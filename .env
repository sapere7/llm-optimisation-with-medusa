# Model settings
MODEL_NAME=lmsys/vicuna-7b-v1.3
MODEL_PRECISION=fp16  # fp16, bf16, int8, or int4
MAX_CONTEXT_LENGTH=2048

# Medusa settings
MEDUSA_CHOICES=3,3,3  # Tree width at each level
MEDUSA_DEPTH=3  # Tree depth
MEDUSA_MODEL_PATH=./medusa_heads.pt  # Path to save/load Medusa heads

# Hugging Face Integration
MEDUSA_HF_REPO=FasterDecoding/medusa-vicuna-7b-v1.3  # 'auto' tries to find a compatible repo, or specify like 'microsoft/Medusa-Vicuna-7B-v1.3'
MEDUSA_HF_SUBFOLDER=  # Subfolder containing Medusa weights, if any
HF_TOKEN= # Hugging Face API token for private repos

# Server settings
MAX_BATCH_SIZE=8
MAX_WAIT_TIME=0.1  # seconds
MAX_QUEUE_SIZE=50
DEFAULT_MAX_TOKENS=512
DEFAULT_TEMPERATURE=0.7

# DeepSpeed settings
DS_ZERO_STAGE=0  # 0, 1, 2, or 3
DS_OFFLOAD_PARAM=false  # true or false
DS_OFFLOAD_OPTIMIZER=false  # true or false
DS_INFERENCE_THREADS=4

# Memory management
MAX_GPU_MEMORY=0  # 0 for all available memory, or specify in GB
